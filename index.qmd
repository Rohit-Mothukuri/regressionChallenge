---
title: "Regression & Interpretability Challenge"
subtitle: "Don't Trust Linear Models - The Perils of Non-Linearity"
format:
  html: default
execute:
  echo: false
  eval: true
---

# Regression Interpretability Analysis

## Introduction

I investigated the dangers of trusting linear regression models when relationships are non-linear. I analyzed a scenario where researchers study the relationship between social media use and anxiety, controlling for stress levels. The true relationship is known: $Anxiety = Stress + 0.1 \times Time$, where Stress is measured by blood tests and Time represents minutes on social media.

However, in practice, researchers often use survey-based proxies (StressSurvey) instead of expensive blood tests. I found that even seemingly good proxy variables can lead to completely misleading regression results when the relationship between the proxy and the true variable is non-linear.

```{python}
#| echo: false
#| include: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import statsmodels.api as sm

# Set style for professional plots
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (8, 5)

# Generate the "true" data with known relationships
observDF = pd.DataFrame({
    'Stress': [0, 0, 0, 1, 1, 1, 2, 2, 2, 8, 8, 8, 12, 12, 12],
    'StressSurvey': [0, 0, 0, 3, 3, 3, 6, 6, 6, 9, 9, 9, 12, 12, 12],
    'Time': [0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2.1, 2.2, 2.2, 2.2],
    'Anxiety': [0, 0.1, 0.1, 1.1, 1.1, 1.1, 2.2, 2.2, 2.2, 8.2, 8.2, 8.21, 12.22, 12.22, 12.22]
})

# True coefficients
true_intercept = 0
true_stress_coef = 1
true_time_coef = 0.1
```

```{python}
#| label: tbl-observations
#| tbl-cap: "Observed data with known true relationships"
observDF
```

The data shows that $Anxiety = Stress + 0.1 \times Time$ holds perfectly. The StressSurvey column represents survey-based measurements that serve as a proxy for actual stress levels measured by blood tests.

## Question 1: Bivariate Regression Analysis with StressSurvey

**Question:** Run a bivariate regression of Anxiety on StressSurvey. What are the estimated coefficients? How do they compare to the true relationship?

```{python}
#| echo: false
# Bivariate regression: Anxiety ~ StressSurvey
X_stress_survey = observDF[['StressSurvey']]
y = observDF['Anxiety']

# Using statsmodels for detailed output
X_with_const = sm.add_constant(X_stress_survey)
model1 = sm.OLS(y, X_with_const).fit()

print("Bivariate Regression: Anxiety ~ StressSurvey")
print("=" * 60)
print(model1.summary())
print("\n" + "=" * 60)
print(f"\nEstimated Intercept: {model1.params['const']:.4f}")
print(f"Estimated StressSurvey Coefficient: {model1.params['StressSurvey']:.4f}")
print(f"R-squared: {model1.rsquared:.4f}")
print(f"\nNote: True relationship is Anxiety = Stress + 0.1*Time")
print(f"      StressSurvey is a proxy for Stress, not the true Stress variable")
```

**Answer:**

The regression estimates a StressSurvey coefficient of 1.0470 with R² = 0.9011. This suggests a strong relationship: each one-unit increase in survey-reported stress corresponds to a 1.047-unit increase in anxiety.

However, this result is misleading. The true relationship is $Anxiety = Stress + 0.1 \times Time$, where Stress is measured by blood tests. StressSurvey is a non-linear proxy for true Stress—at low levels it underreports, at high levels it overreports. The coefficient of 1.0470 reflects how anxiety changes with survey responses, not with actual stress levels.

The high R² is deceptive: it shows a strong correlation but doesn't guarantee interpretable coefficients. A model can fit well while telling the wrong story.

## Question 2: Visualization of Bivariate Relationship

**Question:** Create a scatter plot with the regression line showing the relationship between StressSurvey and Anxiety. Comment on the fit and any potential issues.

```{python}
#| label: fig-stress-survey-bivariate
#| fig-cap: "Bivariate relationship between StressSurvey and Anxiety with regression line"
#| echo: false

fig, ax = plt.subplots(figsize=(8, 5))

# Scatter plot
ax.scatter(observDF['StressSurvey'], observDF['Anxiety'], 
           s=100, alpha=0.7, color='steelblue', edgecolors='black', linewidth=1.5)

# Regression line
X_plot = np.linspace(observDF['StressSurvey'].min(), observDF['StressSurvey'].max(), 100)
y_pred = model1.params['const'] + model1.params['StressSurvey'] * X_plot
ax.plot(X_plot, y_pred, 'r-', linewidth=2, label=f'Regression Line (R² = {model1.rsquared:.4f})')

ax.set_xlabel('StressSurvey (Survey Response)', fontsize=12)
ax.set_ylabel('Anxiety Level', fontsize=12)
ax.set_title('Bivariate Relationship: StressSurvey vs Anxiety', fontsize=14, fontweight='bold')
ax.legend(fontsize=10)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

**Answer:**

The plot shows a strong linear relationship with R² = 0.9011. Higher StressSurvey scores clearly correspond to higher Anxiety levels, and the regression line fits the data well.

However, this visual fit is misleading. The points cluster at discrete StressSurvey values rather than forming a smooth distribution, revealing that StressSurvey is not measured on a continuous linear scale. The model assumes linearity, but the underlying relationship between StressSurvey and true Stress is non-linear. The regression line looks convincing, but it doesn't capture how Anxiety actually relates to stress—only how it relates to survey responses.

## Question 3: Bivariate Regression Analysis with Time

**Question:** Run a bivariate regression of Anxiety on Time. What are the estimated coefficients? How do they compare to the true relationship?

```{python}
#| echo: false
# Bivariate regression: Anxiety ~ Time
X_time = observDF[['Time']]
X_time_const = sm.add_constant(X_time)
model2 = sm.OLS(y, X_time_const).fit()

print("Bivariate Regression: Anxiety ~ Time")
print("=" * 60)
print(model2.summary())
print("\n" + "=" * 60)
print(f"\nEstimated Intercept: {model2.params['const']:.4f}")
print(f"Estimated Time Coefficient: {model2.params['Time']:.4f}")
print(f"R-squared: {model2.rsquared:.4f}")
print(f"\nTrue Time Coefficient: {true_time_coef}")
print(f"Difference from true: {abs(model2.params['Time'] - true_time_coef):.4f}")
```

**Answer:**

The regression estimates a Time coefficient of 5.3406 with R² = 0.5630. However, the true relationship is $Anxiety = Stress + 0.1 \times Time$, where the Time coefficient should be 0.1.

The estimate is inflated by a factor of 53 because Stress is omitted from the model. Since Stress increases across the dataset, the regression mistakenly attributes Stress's effect to Time. This is classic omitted variable bias: the Time coefficient absorbs the missing Stress effect, making it appear far larger than it actually is.

Even with statistical significance, this coefficient doesn't represent the true effect of Time on Anxiety. Bivariate regressions can be misleading when important variables are omitted.

## Question 4: Visualization of Bivariate Relationship

**Question:** Create a scatter plot with the regression line showing the relationship between Time and Anxiety. Comment on the fit and any potential issues.

```{python}
#| label: fig-time-bivariate
#| fig-cap: "Bivariate relationship between Time and Anxiety with regression line"
#| echo: false

fig, ax = plt.subplots(figsize=(8, 5))

# Scatter plot
ax.scatter(observDF['Time'], observDF['Anxiety'], 
           s=100, alpha=0.7, color='darkgreen', edgecolors='black', linewidth=1.5)

# Regression line
X_plot_time = np.linspace(observDF['Time'].min(), observDF['Time'].max(), 100)
y_pred_time = model2.params['const'] + model2.params['Time'] * X_plot_time
ax.plot(X_plot_time, y_pred_time, 'r-', linewidth=2, label=f'Regression Line (R² = {model2.rsquared:.4f})')

ax.set_xlabel('Time (Minutes on Social Media)', fontsize=12)
ax.set_ylabel('Anxiety Level', fontsize=12)
ax.set_title('Bivariate Relationship: Time vs Anxiety', fontsize=14, fontweight='bold')
ax.legend(fontsize=10)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

**Answer:**

The plot shows an upward trend with R² = 0.5630. As Time increases, Anxiety increases, but the points are widely scattered around the regression line.

This scatter reveals the problem: Time alone doesn't explain Anxiety well because Stress is omitted. Since Stress and Time both increase across the dataset, the regression mistakenly attributes Stress's effect to Time, distorting the fitted line and inflating Time's apparent impact. The visual fit looks reasonable, but it captures a confounded relationship rather than Time's true effect.

## Question 5: Multiple Regression Analysis

**Question:** Run a multiple regression of Anxiety on both StressSurvey and Time. What are the estimated coefficients? How do they compare to the true relationship?

```{python}
#| echo: false
# Multiple regression: Anxiety ~ StressSurvey + Time
X_multi_survey = observDF[['StressSurvey', 'Time']]
X_multi_survey_const = sm.add_constant(X_multi_survey)
model3 = sm.OLS(y, X_multi_survey_const).fit()

print("Multiple Regression: Anxiety ~ StressSurvey + Time")
print("=" * 60)
print(model3.summary())
print("\n" + "=" * 60)
print(f"\nEstimated Intercept: {model3.params['const']:.4f}")
print(f"Estimated StressSurvey Coefficient: {model3.params['StressSurvey']:.4f}")
print(f"Estimated Time Coefficient: {model3.params['Time']:.4f}")
print(f"R-squared: {model3.rsquared:.4f}")
print(f"\nTrue Coefficients:")
print(f"  Intercept: {true_intercept}")
print(f"  Stress Coefficient: {true_stress_coef}")
print(f"  Time Coefficient: {true_time_coef}")
print(f"\nComparison:")
print(f"  StressSurvey coef vs true Stress coef: {model3.params['StressSurvey']:.4f} vs {true_stress_coef}")
print(f"  Time coef vs true Time coef: {model3.params['Time']:.4f} vs {true_time_coef}")
```

**Answer:**

The regression estimates a StressSurvey coefficient of 1.4269 and a Time coefficient of –2.7799, with R² = 0.9350. This suggests a strong statistical fit.

However, the true relationship is $Anxiety = Stress + 0.1 \times Time$, where the Stress coefficient should be 1.0 and the Time coefficient should be 0.1. The estimates are wrong in two critical ways:

First, the StressSurvey coefficient (1.4269) exceeds the true Stress effect (1.0) because StressSurvey is a non-linear proxy that doesn't scale linearly with true Stress.

Second, and more alarming, the Time coefficient flips sign: estimated at –2.7799 when the true effect is +0.1. This happens because StressSurvey absorbs variation in Anxiety in a distorted way, leaving Time to capture whatever residual remains—even if that means producing the wrong sign.

This is the core problem: even with high R² and statistically significant coefficients, the model produces completely wrong conclusions. The non-linear proxy distorts the coefficients so severely that Time appears harmful when it's actually beneficial, and the true relationship is hidden entirely.

## Question 6: Multiple Regression Analysis with True Stress

**Question:** Run a multiple regression of Anxiety on both Stress and Time. What are the estimated coefficients? How do they compare to the true relationship?

```{python}
#| echo: false
# Multiple regression: Anxiety ~ Stress + Time
X_multi_true = observDF[['Stress', 'Time']]
X_multi_true_const = sm.add_constant(X_multi_true)
model4 = sm.OLS(y, X_multi_true_const).fit()

print("Multiple Regression: Anxiety ~ Stress + Time")
print("=" * 60)
print(model4.summary())
print("\n" + "=" * 60)
print(f"\nEstimated Intercept: {model4.params['const']:.4f}")
print(f"Estimated Stress Coefficient: {model4.params['Stress']:.4f}")
print(f"Estimated Time Coefficient: {model4.params['Time']:.4f}")
print(f"R-squared: {model4.rsquared:.4f}")
print(f"\nTrue Coefficients:")
print(f"  Intercept: {true_intercept}")
print(f"  Stress Coefficient: {true_stress_coef}")
print(f"  Time Coefficient: {true_time_coef}")
print(f"\nComparison:")
print(f"  Stress coef vs true: {model4.params['Stress']:.4f} vs {true_stress_coef} (difference: {abs(model4.params['Stress'] - true_stress_coef):.6f})")
print(f"  Time coef vs true: {model4.params['Time']:.4f} vs {true_time_coef} (difference: {abs(model4.params['Time'] - true_time_coef):.6f})")
```

**Answer:**

Using the true Stress variable instead of StressSurvey, the regression recovers the actual relationship perfectly: Stress coefficient = 1.0000, Time coefficient = 0.1000, R² = 1.0000. These match the true formula $Anxiety = Stress + 0.1 \times Time$ exactly.

This demonstrates a crucial distinction: regression works correctly when variables are properly measured. The model with StressSurvey produced inflated coefficients, a negative Time effect, and misleading interpretations. But with the true Stress variable, all those problems disappear.

The failures in earlier regressions were caused by bad measurement and non-linearity, not by regression itself. When relationships are truly linear and variables are measured correctly, regression performs exactly as intended.

## Question 7: Model Comparison

**Question:** Compare the R-squared values and coefficient interpretations between the two multiple regression models. Do both models show statistical significance in all of their coefficient estimates? What does this tell you about the real-world implications of multiple regression results?

```{python}
#| echo: false
# Create comparison table
comparison_data = {
    'Model': ['Anxiety ~ StressSurvey + Time', 'Anxiety ~ Stress + Time'],
    'R-squared': [model3.rsquared, model4.rsquared],
    'Intercept': [model3.params['const'], model4.params['const']],
    'Stress/StressSurvey Coef': [model3.params['StressSurvey'], model4.params['Stress']],
    'Time Coef': [model3.params['Time'], model4.params['Time']],
    'Stress/StressSurvey p-value': [model3.pvalues['StressSurvey'], model4.pvalues['Stress']],
    'Time p-value': [model3.pvalues['Time'], model4.pvalues['Time']]
}

comparison_df = pd.DataFrame(comparison_data)
print("Model Comparison")
print("=" * 80)
print(comparison_df.to_string(index=False))
print("\n" + "=" * 80)
print("\nStatistical Significance (p < 0.05):")
print(f"\nModel 1 (StressSurvey + Time):")
print(f"  StressSurvey: p = {model3.pvalues['StressSurvey']:.2e} {'✓ Significant' if model3.pvalues['StressSurvey'] < 0.05 else '✗ Not Significant'}")
print(f"  Time: p = {model3.pvalues['Time']:.2e} {'✓ Significant' if model3.pvalues['Time'] < 0.05 else '✗ Not Significant'}")
print(f"\nModel 2 (Stress + Time):")
print(f"  Stress: p = {model4.pvalues['Stress']:.2e} {'✓ Significant' if model4.pvalues['Stress'] < 0.05 else '✗ Not Significant'}")
print(f"  Time: p = {model4.pvalues['Time']:.2e} {'✓ Significant' if model4.pvalues['Time'] < 0.05 else '✗ Not Significant'}")
```

```{python}
#| label: fig-model-comparison
#| fig-cap: "Visual comparison of coefficient estimates from both models"
#| echo: false

fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10))

# Top plot: Stress/StressSurvey coefficients
models = ['Model 1\n(StressSurvey)', 'Model 2\n(Stress)']
stress_coefs = [model3.params['StressSurvey'], model4.params['Stress']]
true_stress = [true_stress_coef, true_stress_coef]

x_pos = np.arange(len(models))
width = 0.35

bars1 = ax1.bar(x_pos - width/2, stress_coefs, width, label='Estimated', color='steelblue', alpha=0.7)
bars2 = ax1.bar(x_pos + width/2, true_stress, width, label='True Value', color='red', alpha=0.7)

ax1.set_ylabel('Coefficient Value', fontsize=11)
ax1.set_title('Stress/StressSurvey Coefficients', fontsize=12, fontweight='bold')
ax1.set_xticks(x_pos)
ax1.set_xticklabels(models)
ax1.legend()
ax1.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
ax1.grid(True, alpha=0.3, axis='y')

# Bottom plot: Time coefficients
time_coefs = [model3.params['Time'], model4.params['Time']]
true_time = [true_time_coef, true_time_coef]

bars3 = ax2.bar(x_pos - width/2, time_coefs, width, label='Estimated', color='darkgreen', alpha=0.7)
bars4 = ax2.bar(x_pos + width/2, true_time, width, label='True Value', color='red', alpha=0.7)

ax2.set_ylabel('Coefficient Value', fontsize=11)
ax2.set_title('Time Coefficients', fontsize=12, fontweight='bold')
ax2.set_xticks(x_pos)
ax2.set_xticklabels(models)
ax2.legend()
ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
ax2.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()
```

**Answer:**

Both models show high R² and statistically significant coefficients, yet they produce dramatically different results.

**Model 1 (StressSurvey + Time):** R² = 0.935, StressSurvey coefficient = 1.4269 (inflated), Time coefficient = –2.7799 (wrong sign). Both coefficients are significant.

**Model 2 (Stress + Time):** R² = 1.000, Stress coefficient = 1.0000, Time coefficient = 0.1000. Both coefficients are significant and match the true values exactly.

Model 1 looks statistically strong but produces completely wrong coefficients. StressSurvey overestimates the stress effect, and Time flips from a true positive effect (+0.1) to a strong negative effect (–2.78). This happens because StressSurvey is a non-linear proxy—the model forces linearity and distorts both coefficients.

Model 2, using the true Stress variable, recovers the exact relationship with no distortion.

This comparison reveals a critical lesson: a model can look statistically strong with high R² and significant p-values, yet still give completely wrong conclusions when variables are measured poorly or relationships are non-linear. Statistical significance alone does not guarantee that the model is telling the truth.

## Question 8: Real-World Implications

**Question:** For each of the two multiple regression models, assume their respective outputs/conclusions were published in academic journals and then subsequently picked up by the popular press. What headline about time spent on social media and its effect on anxiety would you expect to see from a popular press outlet covering the first model? And what headline would you expect to see from a popular press outlet covering the second model? Assuming confirmation bias is real, which model is a typical parent going to believe? Which model will Facebook, Instagram, and TikTok executives prefer?

**Answer:**

If published and reported in the news, these models would produce opposite headlines.

**Model 1 (StressSurvey + Time):** With a negative Time coefficient (–2.78), the headline would likely be *"New Study Shows More Time on Social Media Lowers Anxiety Levels."* Because StressSurvey is a poor proxy, the model misrepresents the real effect.

**Model 2 (Stress + Time):** This model correctly finds that stress increases anxiety and time increases anxiety slightly (+0.1). The headline would likely be *"Study Finds Social Media Time Raises Anxiety, Even After Accounting for Stress."*

Parents would likely believe the second headline because it matches their intuition and experience. Social media platforms would prefer the first headline because it shifts blame away from their products.

These two models tell completely different stories despite both being statistically significant. The difference comes entirely from using a bad measurement (StressSurvey) instead of the true Stress variable. This demonstrates how regression can produce convincing but contradictory claims depending on how variables are measured—a real danger when findings spread to the public.

## Question 9: Avoiding Misleading Statistical Significance

**Question:** Reflect on this tip to avoid being misled by statistically significant results: splitting the sample into meaningful subsets ("statistical regimes"), and using graphical diagnostics for linearity rather than blind reliance on "canned" regressions. Apply this approach to multiple regression of Anxiety on both StressSurvey and Time by analyzing a smartly chosen subset of the data. What specific subset did you choose and why? Did you get results that are both statistically significant and close to the true relationship?

```{python}
#| echo: false
# First, let's visualize the relationship between StressSurvey and Stress
# to understand the non-linearity

fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10))

# Top: StressSurvey vs Stress (showing non-linearity)
ax1.plot(observDF['Stress'], observDF['StressSurvey'], 
         'o-', linewidth=2, markersize=10, color='purple')
ax1.set_xlabel('True Stress', fontsize=11)
ax1.set_ylabel('StressSurvey', fontsize=11)
ax1.set_title('Non-Linear Relationship:\nStressSurvey vs True Stress', fontsize=12, fontweight='bold')
ax1.grid(True, alpha=0.3)

# Add linear fit line for comparison
from sklearn.linear_model import LinearRegression
stress_linear = LinearRegression()
stress_linear.fit(observDF[['Stress']], observDF['StressSurvey'])
stress_plot = np.linspace(observDF['Stress'].min(), observDF['Stress'].max(), 100)
ax1.plot(stress_plot, stress_linear.predict(stress_plot.reshape(-1, 1)), 
         'r--', linewidth=1.5, alpha=0.7, label='Linear Approximation')
ax1.legend()

# Bottom: Residuals plot to identify regimes
# Calculate residuals from the full model
y_pred_full = model3.predict(X_multi_survey_const)
residuals = y - y_pred_full

ax2.scatter(observDF['StressSurvey'], residuals, s=100, alpha=0.7, color='orange', edgecolors='black')
ax2.axhline(y=0, color='red', linestyle='--', linewidth=2)
ax2.set_xlabel('StressSurvey', fontsize=11)
ax2.set_ylabel('Residuals', fontsize=11)
ax2.set_title('Residuals Plot:\nIdentifying Non-Linear Patterns', fontsize=12, fontweight='bold')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Identify a subset where the relationship is more linear
# Looking at the data, we can see that StressSurvey has distinct clusters
# Let's try using only the lower stress levels where the relationship might be more linear

print("\nAnalyzing different subsets of the data...")
print("=" * 80)
```

```{python}
#| echo: false
# Strategy: Split by StressSurvey levels to create more linear regimes
# Looking at the data, we see clusters at StressSurvey = 0, 3, 6, 9, 12
# The non-linearity is most pronounced when we include all levels
# Let's try using only the lower stress levels (StressSurvey <= 6)

subset_mask = observDF['StressSurvey'] <= 6
subset_df = observDF[subset_mask].copy()

print(f"Subset chosen: StressSurvey <= 6")
print(f"Number of observations in subset: {len(subset_df)}")
print(f"Number of observations in full dataset: {len(observDF)}")
print(f"\nSubset data:")
print(subset_df[['Stress', 'StressSurvey', 'Time', 'Anxiety']])
```

```{python}
#| echo: false
# Run multiple regression on the subset
X_subset = subset_df[['StressSurvey', 'Time']]
y_subset = subset_df['Anxiety']
X_subset_const = sm.add_constant(X_subset)
model_subset = sm.OLS(y_subset, X_subset_const).fit()

print("\nMultiple Regression on Subset: Anxiety ~ StressSurvey + Time (StressSurvey <= 6)")
print("=" * 80)
print(model_subset.summary())
print("\n" + "=" * 80)
print(f"\nEstimated Intercept: {model_subset.params['const']:.4f}")
print(f"Estimated StressSurvey Coefficient: {model_subset.params['StressSurvey']:.4f}")
print(f"Estimated Time Coefficient: {model_subset.params['Time']:.4f}")
print(f"R-squared: {model_subset.rsquared:.4f}")
print(f"\nTrue Coefficients:")
print(f"  Intercept: {true_intercept}")
print(f"  Stress Coefficient: {true_stress_coef}")
print(f"  Time Coefficient: {true_time_coef}")
print(f"\nComparison:")
print(f"  StressSurvey coef vs true Stress coef: {model_subset.params['StressSurvey']:.4f} vs {true_stress_coef} (difference: {abs(model_subset.params['StressSurvey'] - true_stress_coef):.4f})")
print(f"  Time coef vs true Time coef: {model_subset.params['Time']:.4f} vs {true_time_coef} (difference: {abs(model_subset.params['Time'] - true_time_coef):.4f})")
print(f"\nStatistical Significance:")
print(f"  StressSurvey: p = {model_subset.pvalues['StressSurvey']:.4f} {'✓ Significant' if model_subset.pvalues['StressSurvey'] < 0.05 else '✗ Not Significant'}")
print(f"  Time: p = {model_subset.pvalues['Time']:.4f} {'✓ Significant' if model_subset.pvalues['Time'] < 0.05 else '✗ Not Significant'}")
```

```{python}
#| label: fig-subset-analysis
#| fig-cap: "Comparison of coefficient estimates: full model vs subset model"
#| echo: false

fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10))

# Top: StressSurvey coefficients
models_compare = ['Full Model\n(All Data)', 'Subset Model\n(StressSurvey ≤ 6)']
stress_coefs_compare = [model3.params['StressSurvey'], model_subset.params['StressSurvey']]
true_stress_compare = [true_stress_coef, true_stress_coef]

x_pos = np.arange(len(models_compare))
width = 0.35

bars1 = ax1.bar(x_pos - width/2, stress_coefs_compare, width, label='Estimated', color='steelblue', alpha=0.7)
bars2 = ax1.bar(x_pos + width/2, true_stress_compare, width, label='True Value', color='red', alpha=0.7)

ax1.set_ylabel('Coefficient Value', fontsize=11)
ax1.set_title('StressSurvey Coefficients', fontsize=12, fontweight='bold')
ax1.set_xticks(x_pos)
ax1.set_xticklabels(models_compare)
ax1.legend()
ax1.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
ax1.grid(True, alpha=0.3, axis='y')

# Bottom: Time coefficients
time_coefs_compare = [model3.params['Time'], model_subset.params['Time']]
true_time_compare = [true_time_coef, true_time_coef]

bars3 = ax2.bar(x_pos - width/2, time_coefs_compare, width, label='Estimated', color='darkgreen', alpha=0.7)
bars4 = ax2.bar(x_pos + width/2, true_time_compare, width, label='True Value', color='red', alpha=0.7)

ax2.set_ylabel('Coefficient Value', fontsize=11)
ax2.set_title('Time Coefficients', fontsize=12, fontweight='bold')
ax2.set_xticks(x_pos)
ax2.set_xticklabels(models_compare)
ax2.legend()
ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
ax2.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()
```

**Answer:**

To avoid being misled, I identified a subset where the relationship between StressSurvey and true Stress is more linear. The largest distortions occur at high StressSurvey values (8 and 12), where the survey jumps in big steps.

I chose all observations where StressSurvey ≤ 6, which includes lower categories (0, 3, and 6) where StressSurvey increases in small, consistent increments. This removes the non-linear jumps and makes a linear model more appropriate.

The subset regression (Anxiety ~ StressSurvey + Time) gives: Intercept = 0.0000, StressSurvey coefficient = 0.3333, Time coefficient = 0.1000, R² = 1.000. Both coefficients are highly significant (p < 0.001).

The Time coefficient matches the true value exactly (0.1). The StressSurvey coefficient (0.3333) is lower than the true Stress effect (1.0), but this makes sense: in this subset, StressSurvey rises by 3 units for each 1-unit increase in Stress, so one StressSurvey unit corresponds to one-third of a Stress unit.

By using only the more linear region of the data, the model produces a correct positive effect of Time, avoids the earlier sign flips and coefficient distortions, and gives results that match the true relationship much more closely. This demonstrates why visual diagnostics and thoughtful subsetting are essential—and why blindly trusting full-sample regressions can lead to misleading conclusions.

## Conclusion

Linear regression can produce high R-squared values, tiny p-values, and statistically significant coefficients—even when the model tells the wrong story. The danger isn't the math; it's misunderstanding what the coefficients actually represent.

In this analysis, StressSurvey is a distorted, non-linear proxy for true Stress. The uneven spacing between values (0, 3, 6, 9, 12) makes the scale meaningless. When the model estimates a StressSurvey coefficient of 1.43 and a Time coefficient of –2.78, these numbers don't describe real-world effects—they describe the shape of a broken measurement tool. Even with strong statistical significance, these coefficients don't represent how Anxiety actually responds to stress or time.

The most alarming failure is the sign flip: the true effect of Time is positive (+0.1), but the model estimates a strong negative effect (–2.78). This happens because when a predictor is non-linear, unevenly spaced, or measured with error, the regression tries to correct the misfit by shifting effects into other variables. StressSurvey absorbs the wrong patterns, and Time picks up the leftover distortion. The result is a model that's statistically significant but scientifically false.

This has real-world consequences. A model using StressSurvey would produce the headline "More time on social media lowers anxiety," while a model using true Stress would produce "Social media use increases anxiety, even after accounting for stress." Both claims would be statistically significant, but only one is true. This gap between statistical significance and scientific truth is where public misunderstanding and bad policy emerge.

To avoid these mistakes, analysts must check for non-linearity visually, break data into meaningful subsets where linearity holds better, interpret coefficients cautiously rather than automatically, suspect sign flips as indicators of model misspecification, and measure key variables as directly as possible. The subset with StressSurvey ≤ 6 produced coefficients much closer to the true model, demonstrating the value of thoughtful subsetting.

Linear regression is powerful but dangerously easy to misinterpret. Even significant results can be completely wrong when variables are non-linear, mismeasured, or poorly scaled. The solution isn't more statistics—it's better thinking, better pictures, and better measurement.
